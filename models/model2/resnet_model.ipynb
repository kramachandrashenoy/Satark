{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Add, ZeroPadding2D, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "# Configuration class for model parameters\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 16\n",
    "        self.img_width = 224\n",
    "        self.img_height = 224\n",
    "        self.data_dir = \"../input/train/\"\n",
    "        self.model_save_path = \"resnet_trained_model.h5\"  # Model save path\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# Function to load image dimensions\n",
    "def load_image_dimensions(directory):\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    min_width = 35000\n",
    "    min_height = 35000\n",
    "    img_count = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                img_count += 1\n",
    "                filename = os.path.join(subdir, file)\n",
    "                image = Image.open(filename)\n",
    "                width, height = image.size\n",
    "                if width < min_width:\n",
    "                    min_width = width\n",
    "                if height < min_height:\n",
    "                    min_height = height\n",
    "                if width > max_width:\n",
    "                    max_width = width\n",
    "                if height > max_height:\n",
    "                    max_height = height\n",
    "    \n",
    "    return min_width, min_height, max_width, max_height, img_count\n",
    "\n",
    "# Function to list directory counts\n",
    "def list_directory_counts(directory):\n",
    "    dir_counts = []\n",
    "    for subdir, dirs, files in os.walk(directory, topdown=False):\n",
    "        file_count = len(files)\n",
    "        dirname = subdir\n",
    "        dir_counts.append((dirname, file_count))\n",
    "    return dir_counts\n",
    "\n",
    "# Function to preprocess and split category information\n",
    "def preprocess_categories(df):\n",
    "    for index, row in df.iterrows():\n",
    "        directory = row['Category'].split('/')\n",
    "        if directory[3] != '':\n",
    "            directory = directory[3]\n",
    "            df.at[index, 'Category'] = directory\n",
    "        else:\n",
    "            df.drop(index, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to setup data generators for training and validation\n",
    "def setup_data_generators(train_dir, val_dir, img_width, img_height, batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "    \n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Function to build ResNet50 model architecture\n",
    "def build_resnet50(input_shape=(224, 224, 3), num_classes=10):\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(32, (7, 7), strides=(2, 2), name='conv1')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=5, filters=[32, 32, 128], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 5, [32, 32, 128], stage=2, block='b')\n",
    "    X = identity_block(X, 5, [32, 32, 128], stage=2, block='c')\n",
    "    \n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='d')\n",
    "    \n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='f')\n",
    "    \n",
    "    # Stage 5\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=5, block='c')\n",
    "    \n",
    "    # Average pooling and output layer\n",
    "    X = AveragePooling2D(pool_size=(5, 5), padding='valid')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(num_classes, activation='softmax', name='fc' + str(num_classes))(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "    return model\n",
    "\n",
    "# Function to define convolutional block\n",
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    conv_name_base = 'conv' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F3, (1, 1), strides=(1, 1), name=conv_name_base + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), name=conv_name_base + '1')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Function to define identity block\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    conv_name_base = 'conv' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = Conv2D(F1, (1, 1), name=conv_name_base + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F2, (f, f), padding='same', name=conv_name_base + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F3, (1, 1), name=conv_name_base + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs, batch_size):\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint_path = \"model_checkpoint.h5\"\n",
    "    checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                          save_weights_only=True,\n",
    "                                          monitor='val_accuracy',\n",
    "                                          mode='max',\n",
    "                                          save_best_only=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=train_generator.samples // batch_size,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=validation_generator.samples // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=[checkpoint_callback])\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Save the entire model\n",
    "    model.save(config.model_save_path)\n",
    "    print(f\"Model saved to {config.model_save_path}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, validation_generator):\n",
    "    results = model.evaluate(validation_generator)\n",
    "    return results\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main():\n",
    "    # Load dataset dimensions\n",
    "    min_width, min_height, max_width, max_height, img_count = load_image_dimensions(config.data_dir)\n",
    "    print(f\"Image dimensions - Min Width: {min_width}, Min Height: {min_height}, Max Width: {max_width}, Max Height: {max_height}, Total Images: {img_count}\")\n",
    "    \n",
    "    # List directory counts\n",
    "    dir_counts = list_directory_counts(config.data_dir)\n",
    "    print(f\"Directory Counts: {dir_counts}\")\n",
    "    \n",
    "    # Preprocess and split category information\n",
    "    df = pd.read_csv(\"../input/categories.csv\")\n",
    "    df = preprocess_categories(df)\n",
    "    \n",
    "    # Setup data generators for training and validation\n",
    "    train_generator, validation_generator = setup_data_generators(config.data_dir, config.data_dir, config.img_width, config.img_height, config.batch_size)\n",
    "    \n",
    "    # Build ResNet50 model architecture\n",
    "    model = build_resnet50(input_shape=(config.img_width, config.img_height, 3), num_classes=10)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model(model, train_generator, validation_generator, config.epochs, config.batch_size)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = evaluate_model(trained_model, validation_generator)\n",
    "    print(f\"Evaluation Results: {results}\")\n",
    "    \n",
    "    # Plot accuracy and loss over epochs\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf349c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
